# ì¸ê³µì‹ ê²½ë§

## ì¸ê³µì‹ ê²½ë§

ğŸ¤¨ **ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…**

ëŸ­í‚¤ë°± ì´ë²¤íŠ¸ë¥¼ íŒ¨ì…˜ ë¶„ì•¼ë¡œ í™•ëŒ€ â†’ ì£¼ ê³ ê°ì¸µì´ 20ëŒ€ë¡œ ëŸ­í‚¤ë°± í™•ë¥  ì •í™•ë„ í–¥ìƒ í•„ìš”

### íŒ¨ì…˜ MNIST

ì´ ì¥ì—ì„œëŠ” íŒ¨ì…˜ MNISTë¼ëŠ” ë°ì´í„° ì…‹ì„ ì´ìš©í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤. íŒ¨ì…˜ MNISTëŠ” 10ì¢…ë¥˜ì˜ íŒ¨ì…˜ ì•„ì´í…œìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

**í…ì„œí”Œë¡œì˜ ì¼€ë¼ìŠ¤ íŒ¨í‚¤ì§€ë¥¼ ì„í¬íŠ¸í•˜ê³  íŒ¨ì…˜ MNIST ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì.**

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = \\
keras.datasets.fashion_mnist.load_data()
print(train_input.shape, train_target.shape)
print(test_input.shape, test_target.shape)

# ê²°ê³¼
(60000, 28, 28) (60000,) # 60000ê°œì˜ ì´ë¯¸ì§€ë¡œ ì´ë£¨ì–´ì§ ì´ë¯¸ì§€ëŠ” 28 * 28í¬ê¸°ì— íƒ€ê²Ÿë„ 60000ê°œ
(10000, 28, 28) (10000,) # 10000ê°œì˜ ì´ë¯¸ì§€ëŸ¬ ì´ë£¨ì–´ì§ íƒ€ê²Ÿë˜í•œ 10000ê°œ
```

ìƒ˜í”Œì„ ê·¸ë¦¼ìœ¼ë¡œ ì¶œë ¥í•´ë³´ì.

```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
  axs[i].imshow(train_input[i], cmap='gray_r')
  axs[i].axis('off')

plt.show()
```

<figure><img src="../../.gitbook/assets/6-1.png" alt=""><figcaption></figcaption></figure>

íŒŒì´ì¬ì˜ ë¦¬ìŠ¤íŠ¸ ë‚´í¬ë¥¼ ì‚¬ìš©í•´ì„œ ì²˜ìŒ 10ê°œì˜ ìƒ˜í”Œì˜ íƒ€ê²Ÿ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  í›„ ì¶œë ¥í•œë‹¤.

```python
print([train_target[i] for i in range(10)])

# ê²°ê³¼
[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]
```

<figure><img src="../../.gitbook/assets/6-2.png" alt=""><figcaption></figcaption></figure>

### **ë¡œì§€ìŠ¤í‹± íšŒê·€ë¡œ ë¶„ë¥˜**

ì´ë¯¸ì§€ ê° í”½ì…€ ê°’ 0~~255 -> 0~~1 ë¡œ ì •ê·œí™”í•˜ê³ , reshape()ë©”ì„œë“œ ì´ìš©í•´ 2ì°¨ì› ë°°ì—´ì„ 1ì°¨ì› ë°°ì—´ë¡œ ì¬êµ¬ì„±í•˜ì.

```python
train_scaled = train_input / 255.0
train_scaled2 =  train_scaled.reshape(-1, 28*28)

print(train_scaled.shape, train_scaled2.shape)

# ê²°ê³¼
(60000, 28, 28) (60000, 784)
```

SGDClassifier í´ë˜ìŠ¤ì™€ **cross\_validate í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì´ ë°ì´í„°ì—ì„œ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ì„ í™•ì¸**í•´ë³´ì. ì•„ë˜ ì½”ë“œì—ì„œ **n\_jobs=-1 : ëª¨ë“  CPUí™œìš© ë³‘ë ¬ì²˜ë¦¬**ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

```python
import numpy as np

from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss="log", max_iter=5, random_state = 42)
scores = cross_validate(sc, train_scaled2, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))

# ê²°ê³¼
0.8196000000000001
```

ë°˜ë³µ íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ë„ ì„±ëŠ¥ì— í° í–¥ìƒì€ ê±°ì˜ ì—†ë‹¤.

```python
sc = SGDClassifier(loss="log", max_iter=9, random_state = 42)
scores = cross_validate(sc, train_scaled2, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))

# ê²°ê³¼
0.8303666666666667
sc = SGDClassifier(loss="log", max_iter=20, random_state = 42)
scores = cross_validate(sc, train_scaled2, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))

# ê²°ê³¼
0.8436666666666666
```

### ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸

*   ë¡œì§€ìŠ¤í‹± íšŒê·€ ì²˜ë¦¬ ì´í•´

    * ì´ë¯¸ì§€ í”½ì…€1\~í”½ì…€784ì™€ ê°€ì¤‘ì¹˜ì™€ ì ˆí¸ìœ¼ë¡œ zê°’ ê³„ì‚°
    * 10ê°œ í´ë˜ìŠ¤ë³„ë¡œ ê³„ì‚° í›„ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ê±°ì³ í™•ë¥  ì¶œë ¥

    <figure><img src="../../.gitbook/assets/6-3.png" alt=""><figcaption></figcaption></figure>

### ì¸ê³µ ì‹ ê²½ë§

> **ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë¡œ ìƒë¬¼í•™ì  ë‰´ëŸ°ì—ì„œ ì˜ê°ì„ ì–»ì–´ êµ¬ì„±**

* ì¸ê³µ ì‹ ê²½ë§ì˜ êµ¬ì„±
  * ì…ë ¥ì¸µ(input layer) : x1 \~ x784ë¡œ í‘œí˜„ëœ í”½ì…€ ê°’ ìì²´
  * ì¶œë ¥ì¸µ(output layer) : 10ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•´ ê³„ì‚° ëœ ê°’ì¸ z1\~z10
  * ë‰´ëŸ°(neuron) : zê°’ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ìœ„, ìœ ë‹›(unit)ìœ¼ë¡œë„ í‘œí˜„
*   ë§¤ì¼ˆëŸ¬-í”¼ì¸  ë‰´ëŸ°

    * 1943ë…„ ì›ŒëŸ° ë§¤ì»¬ëŸ¬, ì›”í„° í”¼ì¸  ê°€ ì œì•ˆí•œ ë‰´ëŸ° ëª¨ë¸
    * ìƒë¬¼í•™ì  ë‰´ëŸ° : ìˆ˜ìƒëŒê¸°ë¥¼ í†µí•´ ì‹ í˜¸ë¥¼ ë°›ê³  ì„¸í¬ì²´ì— ëª¨ì€ ë’¤ ì‹ í˜¸ê°€ ì„ê³—ê°’ì— ë„ë‹¬í•˜ë©´ ì¶•ì‚­ ëŒê¸°ë¥¼ í†µí•´ ë‹¤ë¥¸ ì„¸í¬ì— ì‹ í˜¸ ì „ë‹¬
    * ì¸ê³µ ë‰´ëŸ° : ìƒë¬¼í•™ì  ë‰´ëŸ°ì˜ ëª¨ì–‘ì„ ë³¸ëœ¬ ìˆ˜í•™ ë¸

    <figure><img src="../../.gitbook/assets/6-4.png" alt=""><figcaption></figcaption></figure>
*   ìƒˆë¡œìš´ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜

    * ì¸ê³µ ì‹ ê²½ë§ì€ ë‡Œ ì†ì˜ ìƒë¬¼í•™ì  ë‰´ëŸ°ê³¼ ë™ì¼í•œ ê²ƒì€ ì•„ë‹˜
    * ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ í•´ê²° í•˜ì§€ ëª»í•œ ë¬¸ì œì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„
    * í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ì´ìš©í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ë„ ê°€ì¥ ê°„ë‹¨í•œ êµ¬ì¡°ì˜ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸

    <figure><img src="../../.gitbook/assets/6-5.png" alt=""><figcaption></figcaption></figure>

### í…ì„œí”Œë¡œ

* ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
* 2015ë…„ êµ¬ê¸€ì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œ
* 2019ë…„ 2.0 ë²„ì „ ë¦´ë¦¬ì¦ˆ
* ì½”ë©ì—ì„œëŠ” ì´ë¯¸ ì„¤ì¹˜ ë˜ì–´ ìˆìœ¼ë©°, ê°„ë‹¨íˆ ì„í¬íŠ¸ í•´ì„œ ì‚¬ìš©

### **ì¼€ë¼ìŠ¤**

* í…ì„œí”Œë¡œì˜ ê³ ìˆ˜ì¤€ API
* GPUì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë°±ì—”ë“œë¡œ ì‚¬ìš©
* ì§ê´€ì ì´ê³  ì‚¬ìš©í•˜ê¸° í¸í•œ ê³ ìˆ˜ì¤€ API ì œê³µ
* í…ì„œí”Œë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì¼€ë¼ìŠ¤ APIê°€ ë‚´ì¥

### ì¸ê³µ ì‹ ê²½ë§ìœ¼ë¡œ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì!

ëª¨ë¸ì˜ ê²€ì¦ ë°©ë²•ì€ ì–´ë–¤ ê²ƒì´ ìˆì„ê¹Œ?

* ë¡œì§€ìŠ¤í‹± íšŒê·€ : êµì°¨ ê²€ì¦ ì‚¬ìš©
* ì¸ê³µì‹ ê²½ë§ : ê²€ì¦ ì„¸íŠ¸ í™œìš©
  * ë”¥ ëŸ¬ë‹ ë¶„ì•¼ì˜ ë°ì´í„° ì…‹ì€ ì¶©ë¶„íˆ í¬ê¸° ë•Œë¬¸ì— ê²€ì¦ ì ìˆ˜ ì•ˆì •ì 
  * êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ê¸°ì—ëŠ” í›ˆë ¨ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼

ë¨¼ì €, ê²€ì¦ ì„¸íŠ¸ë¥¼ ë¶„ë¦¬í•´ë³´ì.

```python
from sklearn.model_selection import train_test_split
train_scaled3, val_scaled, train_target, val_target = train_test_split( train_scaled2, train_target, test_size=0.2, random_state=42)
print(train_scaled3.shape, train_target.shape)
print(val_scaled. shape, val_target.shape)

# ê²°ê³¼
(48000, 784) (48000,)
(12000, 784) (12000,)
```

í›ˆë ¨ì„¸íŠ¸ë¡œ ëª¨ë¸ í›ˆë ¨

* ë°€ì§‘ì¸µ(dense layer) : 784 x 10 = 7,840ê°œ ì„ ìœ¼ë¡œ ì—°ê²°
* ì™„ì „ì—°ê²°ì¸µ(fully connected layer)ë¼ê³ ë„ í•¨

<figure><img src="../../.gitbook/assets/6-6.png" alt=""><figcaption></figcaption></figure>

ì´ì œ ì¼€ë¼ìŠ¤ **Dense í´ë˜ìŠ¤ ì‚¬ìš© ë°€ì§‘ì¸µ êµ¬ì„±**í•˜ì. **ë§¤ê°œë³€ìˆ˜ë¡œëŠ” ë‰´ëŸ° ê°œìˆ˜, ë‰´ëŸ° ì¶œë ¥ ì ìš© í•¨ìˆ˜, ì…ë ¥ í¬ê¸°**ë“±ì´ ìˆë‹¤.

```python
dense = keras.layers.Dense(10, activation="softmax", input_shape=(784,))
```

**ì¼€ë¼ìŠ¤ì˜ Sequentialí´ë˜ìŠ¤ ì‚¬ìš© ëª¨ë¸ êµ¬ì„±**í•˜ì.

* ë°€ì§‘ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±
* í™œì„±í™”í•¨ìˆ˜(activation function) : ë‰´ëŸ°ì˜ ì„ í˜•ë°©ì •ì‹ ê³„ì‚° ê²°ê³¼ì— ì ìš©í•˜ëŠ” í•¨ìˆ˜(ex: softmax)

```python
model = keras.Sequential(dense)
```

<figure><img src="../../.gitbook/assets/6-7.png" alt=""><figcaption></figcaption></figure>

**ì†Œí”„íŠ¸ë§¥ìŠ¤ì™€ ê°™ì´ ë‰´ëŸ°ì˜ ì„ í˜• ë°©ì •ì‹ ê³„ì‚° ê²°ê³¼ì— ì ìš©ë˜ëŠ” í•¨ìˆ˜ë¥¼ í™œì„±í™” í•¨ìˆ˜** ë¼ê³ í•œë‹¤.

### **ì¸ê³µ ì‹ ê²½ë§ìœ¼ë¡œ íŒ¨ì…˜ ì•„ì´í…œ ë¶„ë¥˜í•˜ê¸°**

ì¼€ë¼ìŠ¤ ëª¨ë¸ í›ˆë ¨ì„ í•´ë³´ì. compile()ë©”ì„œë“œ ìˆ˜í–‰ í•´ì•¼ í•˜ë©°, ì†ì‹¤ í•¨ìˆ˜ ì¢…ë¥˜ ì§€ì •, í›ˆë ¨ ê³¼ì •ì—ì„œ ê³„ì‚°í•˜ê³ ì í•˜ëŠ” ì¸¡ì • ê°’ ì§€ì •ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤.

```python
model.compile(loss="sparse_categorical_crossentropy", metrics="accuracy")
# sparseê°€ ë¶™ì€ ì´ìœ ëŠ”? 
# íƒ€ê¹ƒê°’ì„ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë°”ê¾¸ì§€ ì•Šê³  ì‚¬ìš©
# íƒ€ê¹ƒê°’ì´ ì›-í•« ì¸ì½”ë”© ëœ ê°’ì´ë¼ë©´ loss=â€˜categorical_crossentropyâ€™
print(train_target[:10])
# ê²°ê³¼ 
[7 3 5 8 6 9 3 3 9 9]
```

<figure><img src="../../.gitbook/assets/6-8.png" alt=""><figcaption></figcaption></figure>

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨ í•´ë³´ì.

```python
model.fit(train_scaled3, train_target, epochs=5)

# ê²°ê³¼
Epoch 1/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.6058 - accuracy: 0.7972
Epoch 2/5
1500/1500 [==============================] - 2s 1ms/step - loss: 0.4748 - accuracy: 0.8394
Epoch 3/5
1500/1500 [==============================] - 2s 2ms/step - loss: 0.4506 - accuracy: 0.8493
Epoch 4/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.4379 - accuracy: 0.8515
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4301 - accuracy: 0.8540
```

ê·¸ëŸ¼ ì•ì„œ ë”°ë¡œ ë–¼ì–´ ë†“ì€ ê²€ì¦ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸ í•´ë³´ì. \*\*ì¼€ë¼ìŠ¤ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë©”ì„œë“œëŠ” evaluate()\*\*ì´ë‹¤.

```python
model.evaluate(val_scaled, val_target)

# ê²°ê³¼ 
375/375 [==============================] - 1s 3ms/step - loss: 0.4429 - accuracy: 0.8507
[0.4428718388080597, 0.8506666421890259]
```

## 6ì£¼ì°¨ 2ë²ˆì§¸ ìˆ˜ì—…

ğŸ¤¨ **ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…**

íŠ¹ë³„í•œ ë¬¸ì œ x ì‹¤ìŠµ ìœ„ì£¼

### ì‹¤ìŠµì„ ì§„í–‰í•˜ì!

```python
from tensorflow import keras
(train_input, train_target), (test_input, test_target) =\\
keras.datasets.fashion_mnist.load_data()
```

ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ì„ 0~~255 ë²”ìœ„ì—ì„œ 0~~1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ê³ , 28 \* 28 í¬ê¸°ì˜ 2ì°¨ì› ë°°ì—´ì„ 784 í¬ê¸°ì˜ 1ì°¨ì› ë°°ì—´ë¡œ í¼ì¹œë‹¤. ê·¸ í›„ ì‚¬ì´í‚·ëŸ°ì˜ train\_test\_split() í•¨ìˆ˜ë¡œ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë‚˜ëˆˆë‹¤.

```python
from sklearn.model_selection import train_test_split

train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size = 0.2, random_state = 42
)

print(train_scaled.shape, train_target.shape, val_scaled.shape, val_target.shape)

# ê²°ê³¼ 
(48000, 784) (48000,) (12000, 784) (12000,)
```

### 2ê°œì˜ ì¸µ

ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ êµ¬ì„±

* ì€ë‹‰ì¸µ(hidden layer) : ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì— ìˆëŠ” ëª¨ë“ ì¸µ
* 2ê°œ ì¸µ ì¶”ê°€ â€“ ì€ë‹‰ì¸µ 1ê°œ, ì¶œë ¥ì¸µ 1ê°œ
* **keras.layers.Dense()í•¨ìˆ˜ ì‚¬ìš©**

```python
dense1 = keras.layers.Dense(100, activation="sigmoid", input_shape=(784,))
dense2 = keras.layers.Dense(10, activation="softmax")
```

**dense1 : ì€ë‹‰ì¸µ -> 100ê°œ ë‰´ëŸ°, dense2 : ì¶œë ¥ì¸µ -> 10ê°œ ë‰´ëŸ° ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ê°€ ì¶œë ¥ì¸µ ë‰´ëŸ° ê°œìˆ˜ë³´ë‹¤ ë§ì•„ì•¼ í•œë‹¤.**

ì¶œë ¥ì¸µì— ì ìš©í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ëŠ” ì´ì§„ ë¶„ë¥˜ì¼ ê²½ìš° ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ë‹¤ì¤‘ ë¶„ë¥˜ì¼ ê²½ìš° ì†Œí”„íŠ¸ ë§¥ìŠ¤ í•¨ìˆ˜ë¡œ ì •í•´ì ¸ ìˆì§€ë§Œ **ì€ë‹‰ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ëŠ” ë¹„êµì  ììœ ë¡­ê²Œ ì„ íƒì´ ê°€ëŠ¥**í•˜ë‹¤. ex) ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜, ë ë£¨ í•¨ìˆ˜ ë“±

ê·¸ë ‡ë‹¤ë©´, ì€ë‹‰ì¸µì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

* ì€ë‹‰ì¸µì˜ ê³„ì‚° ê²°ê³¼ì— ì˜ë¯¸ë¥¼ ì¤„ ìˆ˜ ìˆì–´ì•¼ í•¨
* ì˜ë¯¸ë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•´ ì„ í˜• ê³„ì‚°ì„ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë¹„í‹€ì–´ ì£¼ì–´ì•¼ í•¨

### ì‹¬ì¸µ ì‹ ê²½ë§ ë§Œë“¤ê¸°

ì´ì œ ì•ì—ì„œ ë§Œë“  dense1ê³¼ dense2 ê°ì²´ë¥¼ Sequential í´ë˜ìŠ¤ì— ì¶”ê°€í•˜ì—¬ **ì‹¬ì¸µ ì‹ ê²½ë§**ì„ êµ¬ì„±í•´ë³´ì.

```python
model = keras.Sequential([dense1, dense2])
```

**Sequential í´ë˜ìŠ¤**ì˜ ê°ì²´ë¥¼ ë§Œë“¤ ë•Œ **ì—¬ëŸ¬ ê°œì˜ ì¸µì„ ì¶”ê°€í•˜ë ¤ë©´ ì´ì™€ ê°™ì´ dense1ê³¼ dense2ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤.**

ì—¬ê¸°ì„œ ì£¼ì˜ í•  ì ì€ ì¶œë ¥ì¸µì„ ê°€ì¥ ë§ˆì§€ë§‰ì— ë‘ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

**ì‹¬ì¸µ ì‹ ê²½ë§**

* 1ê°œ ì´ìƒì˜ ì€ë‹‰ì¸µì„ ë‘ì–´ ì—¬ëŸ¬ ê°œì˜ layerë¡œ ì‹ ê²½ë§ êµ¬ì„±
* ì¶œë ¥ì¸µì€ ê°€ì¥ ë§ˆì§€ë§‰ì— ë‘ì–´ì•¼ í•œë‹¤.
* ì¸ê³µ ì‹ ê²½ë§ì˜ ê°•ë ¥í•œ ì„±ëŠ¥ : ì¸µì„ ì¶”ê°€í•˜ì—¬ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ì—°ì†ì ì¸ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ëŠ¥ë ¥ì—ì„œ ë‚˜ì˜´

**summary() ë©”ì„œë“œ**

* ì¸µë§ˆë‹¤ ì¸µ ì´ë¦„, í´ë˜ìŠ¤, ì¶œë ¥ í¬ê¸°, ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
* ì¸µì— ëŒ€í•œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
* ì´ ëª¨ë¸ íŒŒë¼ë¯¸í„° : ì€ë‹‰ì¸µ íŒŒë¼ë¯¸í„° + ì¶œë ¥ì¸µ íŒŒë¼ë¯¸í„°
* ê°„í˜¹ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ í›ˆë ¨ ë˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ì¸µì´ ìˆë‹¤.

summary() ë©”ì„œë“œë¥¼ ì´ìš©í•´ ì¸µì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œì•„ë³´ì

```python
model.summary()

# ê²°ê³¼
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_4 (Dense)             (None, 100)               78500     
                                                                 
 dense_5 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

### ì¸µì„ ì¶”ê°€í•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•

**Sequential í´ë˜ìŠ¤ì— ì¸µ ì¶”ê°€ ë°©ë²•**

* **Sequentialí´ë˜ìŠ¤** ìƒì„±ì ì•ˆì—ì„œ **Dense í´ë˜ìŠ¤ ê°ì²´ ìƒì„±**

```python
model = keras.Sequential ([
    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),
    keras.layers.Dense(10, activation='softmax', name='output'),
], name='Fashion_MNIST_DNN_Model')

model.summary()

# ê²°ê³¼ 
Model: "Fashion_MNIST_DNN_Model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 hidden (Dense)              (None, 100)               78500     
                                                                 
 output (Dense)              (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

ì´ ë°©ë²•ì€ í¸ë¦¬í•˜ê¸´ í•˜ì§€ë§Œ, ì•„ì£¼ ë§ì€ ì¸µì„ ì¶”ê°€í•˜ë ¤ë©´ Sequential í´ë˜ìŠ¤ ìƒì„±ìê°€ ë§¤ìš° ê¸¸ì–´ì§„ë‹¤. ë˜ ì¡°ê±´ì— ë”°ë¼ ì¸µì„ ì¶”ê°€í•  ìˆ˜ë„ ì—†ë‹¤. **Sequential í´ë˜ìŠ¤ì—ì„œ ì¸µì„ ì¶”ê°€í•  ë•Œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ëª¨ë¸ì˜ add() ë©”ì„œë“œ**ì´ë‹¤.

```python
model = keras.Sequential (name='Fashion_MNIST_DNN_Model2')
model.add(keras. layers. Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'))
model.add(keras. layers. Dense (10, activation='softmax', name='output'))

model.summary()

# ê²°ê³¼
Model: "Fashion_MNIST_DNN_Model2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 hidden (Dense)              (None, 100)               78500     
                                                                 
 output (Dense)              (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨ í•´ë³´ì.

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
Epoch 1/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.5698 - accuracy: 0.8058
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4110 - accuracy: 0.8522
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3757 - accuracy: 0.8649
Epoch 4/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3543 - accuracy: 0.8713
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3364 - accuracy: 0.8779
<keras.src.callbacks.History at 0x7814e03256c0>
```

ì„±ëŠ¥ì„ ë³´ë©´ ì¸µì´ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œí‚¨ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ìŒìœ¼ë¡œëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” í™œì„±í™” í•¨ìˆ˜ì— ëŒ€í•´ ì•Œì•„ë³´ì.

### ë ë£¨ í•¨ìˆ˜

ì´ˆì°½ê¸° ì¸ê³µ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì—ì„œ ë§ì´ ì‚¬ìš©ëœ í™œì„±í™” í•¨ìˆ˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ë‹¨ì ì´ ì¡´ì¬í•˜ëŠ”ë°, **ì˜¤ë¥¸ìª½ê³¼ ì™¼ìª½ ëìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê·¸ë˜í”„ê°€ ëˆ„ì›Œìˆê¸° ë•Œë¬¸ì— ì˜¬ë°”ë¥¸ ì¶œë ¥ì„ ë§Œë“œëŠ”ë° ì‹ ì†í•œ ëŒ€ì‘ì„ í•˜ì§€ ëª»í•œë‹¤.** íŠ¹íˆ ì¸µì´ ë§ì€ ì‹¬ì¸µ ì‹ ê²½ë§ì¼ìˆ˜ë¡ ê·¸ íš¨ê³¼ê°€ ëˆ„ì ë˜ì–´ í•™ìŠµì„ ë” ì–´ë µê²Œ ë§Œë“¤ì—ˆë‹¤. ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ ì œì•ˆ ë˜ì—ˆë‹¤. ë°”ë¡œ **ë ë£¨** ì´ë‹¤. **ë ë£¨ í•¨ìˆ˜ëŠ” ì…ë ¥ì´ ì–‘ìˆ˜ì¼ ê²½ìš° ë§ˆì¹˜ í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ëŠ” ê²ƒì²˜ëŸ¼ ê·¸ëƒ¥ ì…ë ¥ì„ í†µê³¼ ì‹œí‚¤ê³ , ìŒìˆ˜ì¼ ê²½ìš°ì—ëŠ” 0ìœ¼ë¡œ ë§Œë“ ë‹¤.**

ë ë£¨ í•¨ìˆ˜ëŠ” max(0, z)ì™€ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤. zê°€ 0ë³´ë‹¤ í¬ë©´ zë¥¼ ì¶œë ¥í•˜ê³  zê°€ 0ë³´ë‹¤ ì‘ìœ¼ë©´ 0ì„ ì¶œë ¥í•œë‹¤.

### ë ë£¨ í•¨ìˆ˜ì™€ Flattenì¸µ

* Flatten Layer
  * 2ì°¨ì› -> 1ì°¨ì› ë³€í™˜ì„ ìœ„í•œ reshape()ë©”ì„œë“œ ëŒ€ì‹  ì‚¬ìš©
  * ì…ë ¥ ì°¨ì›ì„ ëª¨ë‘ ì¼ë ¬ë¡œ í¼ì¹˜ëŠ” ì—­í•  ìˆ˜í–‰
  * ì…ë ¥ì¸µê³¼ ì€ë‹‰ì¸µ ì‚¬ì´ì— ì¶”ê°€

```python
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
model.summary()

# ê²°ê³¼
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 784)               0         
                                                                 
 dense_6 (Dense)             (None, 100)               78500     
                                                                 
 dense_7 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________

# Flatten í´ë˜ìŠ¤ì— í¬í•¨ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” 0ê°œì´ë‹¤. ì•ì˜ ì¶œë ¥ì—ì„œ 784ê°œì˜ ì…ë ¥ì´ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µìœ¼ë¡œ 
# ì „ë‹¬ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
```

ìœ„ì™€ ê°™ì´ Flatten í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ì…ë ¥ ê°’ì˜ ì°¨ì›ì„ ì§ì‘í•  ìˆ˜ ìˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ ì´ì œ í›ˆë ¨ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì¤€ë¹„í•´ì„œ ëª¨ë¸ì„ í›ˆë ¨ í•´ë³´ì. ì—¬ê¸°ì„œëŠ” **reshape ë©”ì„œë“œ**ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

```python
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape, val_scaled.shape, val_target.shape)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
```

ê·¸ ë‹¤ìŒìœ¼ë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  í›ˆë ¨í•˜ì

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
Epoch 1/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.5329 - accuracy: 0.8112
Epoch 2/5
1500/1500 [==============================] - 6s 4ms/step - loss: 0.3897 - accuracy: 0.8603
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3519 - accuracy: 0.8724
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3325 - accuracy: 0.8807
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3167 - accuracy: 0.8853
<keras.src.callbacks.History at 0x7814cd2c68f0>
```

ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ë³´ë‹¤ í¬ì§€ëŠ” ì•Šì§€ë§Œ ì„±ëŠ¥ì´ ì¡°ê¸ˆ í–¥ìƒë˜ì—ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²€ì¦ ì„¸íŠ¸ì—ì„œë„ ì„±ëŠ¥ì„ í™•ì¸í•´ë³´ì

```python
model.evaluate(val_scaled, val_target)

# ê²°ê³¼
375/375 [==============================] - 1s 3ms/step - loss: 0.3683 - accuracy: 0.8748
[0.36833277344703674, 0.874750018119812]
```

### ì˜µí‹°ë§ˆì´ì €

**í•˜ì´í¼ íŒŒë¼ë¯¸í„°**

* ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ì•Šì•„ ì‚¬ëŒì´ ì§€ì •í•´ ì¤˜ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°
  * ì€ë‹‰ì¸µ ê°œìˆ˜, ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜
  * í™œì„±í™” í•¨ìˆ˜
  * ì¸µì˜ ì¢…ë¥˜
  * fit()ë©”ì„œë“œì˜ batch\_size ë§¤ê°œë³€ìˆ˜, epochs ë§¤ê°œë³€ìˆ˜
  * ì˜µí‹°ë§ˆì´ì €

ì˜µí‹°ë§ˆì´ì €(optimizer)

* compile()ë©”ì„œë“œì—ì„œ ì§€ì •í•˜ëŠ” ê²½ì‚¬ í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ ì¢…ë¥˜
  * ì¼€ë¼ìŠ¤ ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ : RMSprop
  * ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²• ì˜µí‹°ë§ˆì´ì € : SGD, ëª¨ë©˜í…€, ë„¤ìŠ¤í…Œë¡œí”„ ëª¨ë©˜í…€
  * ì ì‘ì  í•™ìŠµë¥  ì˜µí‹°ë§ˆì´ì € : RMSprop, Adagrad
  * ëª¨ë©˜í…€ ìµœì í™”ì™€ RMSpropì˜ ì¥ì  ì ‘ëª© : Adam
  * learning\_rate : í•™ìŠµë¥  ì¡°ì • í•˜ì´í¼ íŒŒë¼ë¯¸í„°

ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‹¤ìŠµ í•´ë³´ì

1. SGD ì˜µí‹°ë§ˆì´ì € ì‹¤ìŠµ

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = \\
keras.datasets. fashion_mnist. load_data()

train_scaled = train_input / 255.0
train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)
print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)
model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))
#model.compile(optimizer='sad', loss='sparse_categorical_crossentropy', metrics='accuracy' )
sgd = keras.optimizers.SGD()
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.8075 - accuracy: 0.7366
Epoch 2/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.5416 - accuracy: 0.8185
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4906 - accuracy: 0.8325
Epoch 4/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4621 - accuracy: 0.8415
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4437 - accuracy: 0.8474
<keras.src.callbacks.History at 0x79f19da7f490>
```

1. SGD ì˜µí‹°ë§ˆì´ì € ì‹¤ìŠµ â€“ í•™ìŠµë¥ 

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

sgd = keras.optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 6s 4ms/step - loss: 0.5598 - accuracy: 0.8009
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4162 - accuracy: 0.8494
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3763 - accuracy: 0.8623
Epoch 4/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3507 - accuracy: 0.8711
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3332 - accuracy: 0.8774
<keras.src.callbacks.History at 0x7814e6dfa770>
```

1. SGD ì˜µí‹°ë§ˆì´ì € - ëª¨ë©˜í…€

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

sgd = keras.optimizers.SGD(momentum=0.9)
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 7s 4ms/step - loss: 0.5641 - accuracy: 0.7992
Epoch 2/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.4154 - accuracy: 0.8501
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3771 - accuracy: 0.8639
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3531 - accuracy: 0.8715
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3342 - accuracy: 0.8773
<keras.src.callbacks.History at 0x79f19d63f820>
```

1. SGD ì˜µí‹°ë§ˆì´ì € â€“ ë„¤ìŠ¤í…Œë¡œí”„ ëª¨ë©˜íŠ¸

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.5376 - accuracy: 0.8133
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4095 - accuracy: 0.8543
Epoch 3/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3733 - accuracy: 0.8649
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3485 - accuracy: 0.8734
Epoch 5/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3304 - accuracy: 0.8795
<keras.src.callbacks.History at 0x79f1a0cf11b0>
```

1. RMSprop ì˜µí‹°ë§ˆì´ì €

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.5338 - accuracy: 0.8123
Epoch 2/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3908 - accuracy: 0.8587
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3520 - accuracy: 0.8717
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3303 - accuracy: 0.8809
Epoch 5/5
1500/1500 [==============================] - 5s 4ms/step - loss: 0.3150 - accuracy: 0.8873
<keras.src.callbacks.History at 0x7814e6abf640>
```

1. Adagrad ì˜µí‹°ë§ˆì´ì €

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 4s 2ms/step - loss: 1.1727 - accuracy: 0.6498
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.7684 - accuracy: 0.7537
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.6838 - accuracy: 0.7800
Epoch 4/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.6389 - accuracy: 0.7954
Epoch 5/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.6094 - accuracy: 0.8033
<keras.src.callbacks.History at 0x79f1a0b975b0>
```

1. Adam ì˜µí‹°ë§ˆì´ì €

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target)  = \\
keras.datasets. fashion_mnist. load_data()
train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split( train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled. shape, train_target.shape, val_scaled. shape, val_target.shape)

model = keras. Sequential()
model.add(keras. layers. Flatten (input_shape=(28,28)))
model.add(keras. layers. Dense(100, activation='relu'))
model.add(keras. layers. Dense (10, activation='softmax'))

adam = keras.optimizers.Adam()
model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics='accuracy' )
model.fit(train_scaled, train_target, epochs=5)

# ê²°ê³¼
(48000, 28, 28) (48000,) (12000, 28, 28) (12000,)
Epoch 1/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.5320 - accuracy: 0.8146
Epoch 2/5
1500/1500 [==============================] - 5s 4ms/step - loss: 0.4005 - accuracy: 0.8558
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3567 - accuracy: 0.8708
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3293 - accuracy: 0.8798
Epoch 5/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.3106 - accuracy: 0.8852
<keras.src.callbacks.History at 0x7814e697e200>
```

ë§ˆì§€ë§‰ìœ¼ë¡œ ê²€ì¦ ì„¸íŠ¸ì—ì„œì˜ ì„±ëŠ¥ í™•ì¸

```python
model.evaluate(val_scaled, val_target)

# ê²°ê³¼
375/375 [==============================] - 1s 2ms/step - loss: 0.3511 - accuracy: 0.8737
[0.35108476877212524, 0.8736666440963745]
```

### 6ì£¼ì°¨ 3ë²ˆì§¸ ìˆ˜ì—…

ğŸ¤¨ **ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…**

ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨

### ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹

* ë¨¸ì‹ ëŸ¬ë‹
  * ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
  * ë§¤ê°œë³€ìˆ˜ ì¡°ì •í•˜ê³  í›ˆë ¨í•˜ëŠ” ê³¼ì • ë°˜ë³µ
  * ëª¨ë¸ êµ¬ì¡°ê°€ ì¼ì • ë¶€ë¶„ ê³ ì •ëœ ëŠë‚Œ
* ë”¥ëŸ¬ë‹
  * ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì§ì ‘ ë§Œë“ ë‹¤ëŠ” ëŠë‚Œì´ ê°•í•¨
  * ì¸µì„ ì¶”ê°€í•˜ê³  ì¸µì— ìˆëŠ” ë‰´ëŸ°ì˜ ê°œìˆ˜ì™€ í™œì„±í™” í•¨ìˆ˜ ê²°ì •
  * í”„ë¡œê·¸ë˜ë¨¸ëŠ” ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¢€ ë” ì¹œìˆ™í•œ ëŠë‚Œ

### ì†ì‹¤ ê³¡ì„ 

* History()í´ë˜ìŠ¤ ê°ì²´
  * ì¼€ë¼ìŠ¤ fit()ë©”ì„œë“œëŠ” History()í´ë˜ìŠ¤ ê°ì²´ ë°˜í™˜
  * History ê°ì²´ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ ê³„ì‚°í•œ ì§€í‘œ í¬í•¨(ì†ì‹¤, ì •í™•ë„ ê°’ ë“±)
  * ê·¸ë˜í”„ ì¶œë ¥ì— í™œìš© í•  ìˆ˜ ìˆë‹¤.

ì‹¤ìŠµì„ ìœ„í•´ ì´ì „ ì ˆì—ì„œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ê°™ì´ íŒ¨ì…˜ MNIST ë°ì´í„° ì…‹ì„ ì ì¬í•˜ê³  í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ì.

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = \\
    keras.datasets.fashion_mnist.load_data()

train_scaled = train_input / 255.0

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```

ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ì„ ë§Œë“¤ì. ì´ì „ ì ˆê³¼ ì°¨ì´ë¥¼ ë‘ì–´ ëª¨ë¸ì„ ë§Œë“œëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.

```python
def model_fn(a_layer=None):
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28, 28)))
    model.add(keras.layers.Dense(100, activation='relu'))
    if a_layer:
        model.add(a_layer)
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model
model = model_fn()

model.summary()

# ê²°ê³¼
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_3 (Flatten)         (None, 784)               0         
                                                                 
 dense_6 (Dense)             (None, 100)               78500     
                                                                 
 dense_7 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë¸ì„ í›ˆë ¨í•˜ì§€ë§Œ ì´ë²ˆì—ëŠ” fit() ë©”ì„œë“œì˜ ê²°ê³¼ë¥¼ history ë³€ìˆ˜ì— ë‹´ì•„ ë³´ì

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=5, verbose=0)
```

history ê°ì²´ì—ëŠ” í›ˆë ¨ ì¸¡ì • ê°’ì´ ë‹´ê²¨ ìˆëŠ” history ë”•ì…”ë„ˆë¦¬ê°€ ë“¤ì–´ ìˆë‹¤. ì–´ë–¤ ê°’ì´ ë“¤ì–´ ìˆëŠ”ì§€ í™•ì¸ í•´ë³´ì.

```python
print(history.history.keys())

# ê²°ê³¼
dict_keys(['loss', 'accuracy'])
```

ì†ì‹¤ê³¼ ì •í™•ë„ê°€ í¬í•¨ë˜ì–´ìˆë‹¤. ì¼€ë¼ìŠ¤ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ì„ ê³„ì‚°**í•˜ë©°, **ì •í™•ë„ëŠ” compile ë©”ì„œë“œì—ì„œ metrics ë§¤ê°œë³€ìˆ˜ì— accuracyë¥¼ ì¶”ê°€ í–ˆê¸° ë•Œë¬¸ì— history ì†ì„±ì— í¬í•¨**ë˜ì–´ìˆë‹¤.

**history ì†ì„±ì— í¬í•¨ëœ ì†ì‹¤ê³¼ ì •í™•ë„ëŠ” ì—í¬í¬ë§ˆë‹¤ ê³„ì‚°ëœ ê°’ì´ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ëœ ë‹¨ìˆœí•œ ë¦¬ìŠ¤íŠ¸**ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë§·í”Œë¡œë¦½ì„ ì´ìš©í•´ ê·¸ë˜í”„ë¡œ ê·¸ë¦´ ìˆ˜ ìˆë‹¤.

```python
import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

<figure><img src="../../.gitbook/assets/6-9 (1).png" alt=""><figcaption></figcaption></figure>

```python
plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

<figure><img src="../../.gitbook/assets/6-10.png" alt=""><figcaption></figcaption></figure>

ì—í¬í¬ íšŸìˆ˜ë¥¼ 20ìœ¼ë¡œ ëŠ˜ë ¤ì„œ ëª¨ë¸ í›ˆë ¨ í›„ ê·¸ë˜í”„ ì¶œë ¥

```python
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0)
plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

<figure><img src="../../.gitbook/assets/6-11.png" alt=""><figcaption></figcaption></figure>

### ê²€ì¦ ì†ì‹¤

**ì¸ê³µ ì‹ ê²½ë§ì€ ëª¨ë‘ ì¼ì¢…ì˜ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©**í•˜ê¸° ë•Œë¬¸ì— ë™ì¼í•œ ê°œë…ì´ ì—¬ê¸°ì—ë„ ì ìš©ëœë‹¤. **ì—í¬í¬ì— ë”°ë¥¸ ê³¼ëŒ€ ì í•©ê³¼ ê³¼ì†Œ ì í•©ì„ íŒŒì•…í•˜ë ¤ë©´ í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•œ ì ìˆ˜ ë¿ë§Œ ì•„ë‹ˆë¼ ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ ì ìˆ˜ë„ í•„ìš”**í•˜ë‹¤.

```python
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target))
```

ë°˜í™˜ëœ history.history ë”•ì…”ë„ˆë¦¬ì— ì–´ë–¤ ê°’ì´ ë“¤ì–´ìˆëŠ”ì§€ í‚¤ë¥¼ í™•ì¸í•´ë³´ì.

```python
print(history.history.keys())

# ê²°ê³¼
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
```

ê²€ì¦ ì„¸íŠ¸ì— ëŒ€í•œ ì†ì‹¤ì€ â€˜val\_lossâ€™ì— ìˆìœ¼ë©°, ì •í™•ë„ëŠ” â€˜val\_accuracyâ€™ì— ë“¤ì–´ ìˆì„ ê²ƒì´ë‹¤. ê³¼ëŒ€/ê³¼ì†Œì í•© ë¬¸ì œë¥¼ ì¡°ì‚¬í•˜ê¸°ìœ„í•´ í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì„ í•œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ì.

```python
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

<figure><img src="../../.gitbook/assets/6-12.png" alt=""><figcaption></figcaption></figure>

ì´ˆê¸°ì— ê²€ì¦ ì†ì‹¤ì´ ê°ì†Œí•˜ë‹¤ê°€ ì ì°¨ ìƒìŠ¹í•˜ëŠ” ëª¨ìŠµì´ë‹¤. í›ˆë ¨ ì†ì‹¤ì€ ê¾¸ì¤€íˆ ê°ì†Œí•˜ê¸°ì— ì „í˜•ì ì¸ ê³¼ëŒ€ ì í•© ëª¨ë¸ì´ë‹¤.

### ì˜µí‹°ë§ˆì´ì €ë¥¼ í™œìš©í•´ ì†ì‹¤ë¥ ì„ í™•ì¸í•´ë³´ì!

Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ í™œìš©í•´ ê·¸ë˜í”„ ì¶œë ¥

```python
model = model_fn()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

<figure><img src="../../.gitbook/assets/6-13.png" alt=""><figcaption></figcaption></figure>

ê³¼ëŒ€ ì í•©ì´ ì¤„ì–´ë“¤ì—ˆìœ¼ë©°, ì—í¬í¬ê°€ 10\~11ì •ë„ ê¹Œì§€ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¡œ ì´ì–´ì§„ë‹¤.

### ë“œë¡­ì•„ì›ƒ

*   **ë“œë¡­ì•„ì›ƒì´ë€?**

    * **í›ˆë ¨ ê³¼ì •ì—ì„œ ì¸µì— ìˆëŠ” ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ ë”(ì¶œë ¥0)**
    * **ê³¼ëŒ€ì í•© ë°©ì§€**
    * ë“œë¡­ì•„ì›ƒ í•  ë‰´ëŸ° ê°¯ìˆ˜â€“í•˜ì´í¼ íŒŒë¼ë¯¸í„°
    * **ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ì´ìœ : ì´ì „ ì¸µì˜ ì¼ë¶€ ë‰´ëŸ°ì´ ëœë¤í•˜ê²Œ êº¼ì§€ë©´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼ëŒ€í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ì¤„ì¼ ìˆ˜ ìˆê³  ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•œë‹¤.**
    * ì¼ë¶€ ë‰´ëŸ°ì˜ ì¶œë ¥ì´ ì—†ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê°ì•ˆí•˜ë©´ ì´ ì‹ ê²½ë§ì€ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ì„ ë§Œë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.
    * **keras.layersíŒ¨í‚¤ì§€ Dropoutí´ë˜ìŠ¤ ì‚¬ìš©**

    <figure><img src="../../.gitbook/assets/6-14.png" alt=""><figcaption></figcaption></figure>

    ì•ì„œ ì •ì˜í•œ model\_fn() í•¨ìˆ˜ì— ë“œë¡­ì•„ì›ƒ ê°ì²´ë¥¼ ì „ë‹¬í•˜ì—¬ ì¸µì„ ì¶”ê°€í•´ ë³´ì.

    ```python
    model = model_fn(keras.layers.Dropout(0.3))

    model.summary()

    # ê²°ê³¼
    # 30% ì •ë„ ë“œë¡­ì•„ì›ƒ
    # ë“œë¡­ì•„ì›ƒ ì¸µì€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ”ë° ì´ëŠ” ì¶œë ¥ì„ 0ìœ¼ë¡œ ë§Œë“¤ì§€ë§Œ ë°°ì—´ì˜ í¬ê¸°ë¥¼ ë°”ê¾¸ì§€ëŠ” ì•ŠëŠ”ë‹¤.
    Model: "sequential_7"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     flatten_7 (Flatten)         (None, 784)               0         
                                                                     
     dense_14 (Dense)            (None, 100)               78500     
                                                                     
     dropout (Dropout)           (None, 100)               0         
                                                                     
     dense_15 (Dense)            (None, 10)                1010      
                                                                     
    =================================================================
    Total params: 79510 (310.59 KB)
    Trainable params: 79510 (310.59 KB)
    Non-trainable params: 0 (0.00 Byte)
    _________________________________________________________________
    ```

    í›ˆë ¨ ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ ë¹„êµí•´ë³´ì

    ```python
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
                  metrics='accuracy')

    history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                        validation_data=(val_scaled, val_target))

    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['train', 'val'])
    plt.show()
    ```

<figure><img src="../../.gitbook/assets/6-15.png" alt=""><figcaption></figcaption></figure>

7\~8ë²ˆì§¸ ì—í¬í¬ì— ê²€ì¦ ì†ì‹¤ì´ ìµœì €ê°€ ë‚˜ì™”ë‹¤. ê·¸ ì´ìƒì˜ ì—í¬í¬ëŠ” ê³¼ëŒ€ì í•© ë°œìƒ

### ëª¨ë¸ ì €ì¥ê³¼ ë³µì›

ëª¨ë¸ ì €ì¥ê³¼ ë³µì›ì— ëŒ€í•œ ì‹¤ìŠµì„ ì§„í–‰ í•´ë³´ì

```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=10, verbose=0,
                    validation_data=(val_scaled, val_target))
```

ì¼€ë¼ìŠ¤ ëª¨ë¸ì€ **í›ˆë ¨ëœ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ëŠ” ê°„í¸í•œ save\_weight() ë©”ì„œë“œë¥¼ ì œê³µ**í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì´ **ë©”ì„œë“œëŠ” í…ì„œí”Œë¡œì˜ ì²´í¬í¬ì¸íŠ¸ í¬ë§·ìœ¼ë¡œ ì €ì¥í•˜ì§€ë§Œ íŒŒì¼ì˜ í™•ì¥ìê°€ â€˜h5â€™ì¼ ê²½ìš° HDF5 í¬ë§·ìœ¼ë¡œ ì €ì¥**í•œë‹¤.

```python
model.save_weights('model-weights.h5')
```

ë˜í•œ **ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í•¨ê»˜ ì €ì¥í•˜ëŠ” save() ë©”ì„œë“œ**ë„ ì œê³µí•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì´ ë©”ì„œë“œëŠ” í…ì„œí”Œë¡œì˜ SavedModel í¬ë§·ìœ¼ë¡œ ì €ì¥í•˜ì§€ë§Œ **íŒŒì¼ì˜ í™•ì¥ìê°€ â€˜h5â€™ì¼ ê²½ìš° HDF5 í¬ë§·ìœ¼ë¡œ ì €ì¥**í•©ë‹ˆë‹¤.

```python
model.save('model-whole.h5')
```

ë‘ íŒŒì¼ì´ ì˜ ë§Œë“¤ì–´ ì¡ŒëŠ”ì§€ í™•ì¸ í•´ë³´ì

```python
!ls -al *.h5

# ê²°ê³¼
-rw-r--r-- 1 root root 333320 Apr 18 02:36 model-weights.h5
-rw-r--r-- 1 root root 981176 Apr 18 02:36 model-whole.h5
```

í›ˆë ¨í•˜ì§€ ì•Šì€ ëª¨ë¸ì„ ë§Œë“¤ê³  íŒŒë¼ë¯¸í„° ë¡œë“œ â€“ load\_weights() ì‚¬ìš©í•´ë³´ì

```python
model = model_fn(keras.layers.Dropout(0.3))

model.load_weights('model-weights.h5')

# ê²°ê³¼
Model: "sequential_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_6 (Flatten)         (None, 784)               0         
                                                                 
 dense_12 (Dense)            (None, 100)               78500     
                                                                 
 dropout_2 (Dropout)         (None, 100)               0         
                                                                 
 dense_13 (Dense)            (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```

ì´ë•Œ load\_weights() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” save\_weights() ë©”ì„œë“œë¡œ ì €ì¥í–ˆë˜ ëª¨ë¸ê³¼ ì •í™•íˆ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì–´ì•¼ í•¨.

ê²€ì¦ ì •í™•ë„ë¥¼ í™•ì¸ í•´ë³´ì.

* predict() ë©”ì„œë“œ í™œìš©, ìƒ˜í”Œë§ˆë‹¤ 10ê°œ í´ë˜ìŠ¤ í™•ë¥  ë°˜í™˜
* ê²€ì¦ ì„¸íŠ¸ 12,000ê°œ ìƒ˜í”Œ : predict ê²°ê³¼ëŠ” (12000, 10) í¬ê¸° ë°°ì—´
* 10ê°œ í™•ë¥  ì¤‘ ê°€ì¥ í° ê°’ì„ ê³¨ë¼ íƒ€ê¹ƒ ë ˆì´ë¸”ê³¼ ë¹„êµ

```python
import numpy as np

val_labels = np.argmax(model.predict(val_scaled), axis=-1)
print(np.mean(val_labels == val_target))

# ê²°ê³¼
375/375 [==============================] - 1s 2ms/step
0.8798333333333334
```

**argmax()í•¨ìˆ˜ : ë°°ì—´ì—ì„œ ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ ë°˜í™˜ axis=-1 : ë°°ì—´ì˜ ë§ˆì§€ë§‰ ì°¨ì›ì„ ë”°ë¼ ìµœëŒ€ ê°’ ì„ íƒ**

<figure><img src="../../.gitbook/assets/6-16.png" alt=""><figcaption></figcaption></figure>

ì €ì¥ëœ ëª¨ë¸ + íŒŒë¼ë¯¸í„° ë¡œë“œë¥¼ í•´ë³´ì

```python
model = keras.models.load_model('model-whole.h5')
model.summary()

model.evaluate(val_scaled, val_target)

# ê²°ê³¼
Model: "sequential_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten_7 (Flatten)         (None, 784)               0         
                                                                 
 dense_14 (Dense)            (None, 100)               78500     
                                                                 
 dropout_3 (Dropout)         (None, 100)               0         
                                                                 
 dense_15 (Dense)            (None, 10)                1010      
                                                                 
=================================================================
Total params: 79510 (310.59 KB)
Trainable params: 79510 (310.59 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
375/375 [==============================] - 1s 2ms/step - loss: 0.4304 - accuracy: 0.8469
[0.43043649196624756, 0.846916675567627]
```

ê°™ì€ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë¡œë“œ í–ˆê¸° ë•Œë¬¸ì— ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°€ì§„ë‹¤.

### ì½œë°±

**ì½œë°±ì´ë€?**

* í›ˆë ¨ ê³¼ì • ì¤‘ê°„ì— íƒ€ ì‘ì—… ìˆ˜í–‰ ì²˜ë¦¬
  * keras.callback íŒ¨í‚¤ì§€ í´ë˜ìŠ¤ë“¤
  * fit()ë©”ì„œë“œ callback ë§¤ê°œë³€ìˆ˜ì— ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ì—¬ ì²˜ë¦¬
  * ModelCheckpoint ì½œë°± : ìµœìƒì˜ ê²€ì¦ ì ìˆ˜ë¥¼ ë§Œë“œëŠ” ëª¨ë¸ ì €ì¥
  * save\_best\_only=True : ê°€ì¥ ë‚®ì€ ê²€ì¦ ì†ì‹¤ì„ ë§Œë“œëŠ” ëª¨ë¸ ì§€ì •

ì•„ë˜ëŠ” ì½œë°± ì‹¤ìŠµ ì˜ˆì œì´ë‹¤.

```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5',save_best_only=True)

model.fit(train_scaled, train_target, epochs=20, verbose=0,
          validation_data=(val_scaled, val_target),
          callbacks=[checkpoint_cb])
model = keras.models.load_model('best-model.h5')

model.evaluate(val_scaled, val_target)

# ê²°ê³¼ 
375/375 [==============================] - 1s 2ms/step - loss: 0.3210 - accuracy: 0.8885
[0.3210257887840271, 0.8884999752044678]
```

ModelCheckpoint ì½œë°±ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—í¬í¬ ë§ˆë‹¤ ëª¨ë¸ì€ ì €ì¥í•œë‹¤. ModelCheckpoint í´ë˜ìŠ¤ì˜ ê°ì²´ checkpoint\_cbë¥¼ ë§Œë“ í›„ fit() ë©”ì„œë“œì˜ callbacks ë§¤ê°œë³€ìˆ˜ì— ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ ì „ë‹¬í•œë‹¤. ëª¨ë¸ì´ í›ˆë ¨ ëœ í›„ best-model.h5ì— ìµœìƒì˜ ê²€ì¦ ì ìˆ˜ë¥¼ ë‚¸ ëª¨ë¸ì´ ì €ì¥ëœë‹¤. ê²°ê³¼ì ìœ¼ë¡œ **ModelCheckpoint ì½œë°±ì´ ê°€ì¥ ë‚®ì€ ê²€ì¦ ì ìˆ˜ì˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì €ì¥í•´ì£¼ì–´ì„œ í¸í•˜ë‹¤.** í•˜ì§€ë§Œ ë¬¸ì œê°€ ìˆë‹¤. ì—¬ì „íˆ ì—í¬í¬ë¥¼ 20ë²ˆ ì‹¤í–‰í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì‚¬ì‹¤ **ê²€ì¦ ì ìˆ˜ê°€ ìƒìŠ¹í•˜ê¸° ì‹œì‘í•˜ë©´ ê·¸ ì´í›„ë¶€í„°ëŠ” ê³¼ëŒ€ì í•©ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— í›ˆë ¨ì„ ê³„ì† í•  í•„ìš”ê°€ ì—†ë‹¤.** ì´ë•Œ **í›ˆë ¨ì„ ì¤‘ì§€í•˜ë©´ ì»´í“¨í„° ìì›ê³¼ ì‹œê°„ì„ ì•„ë‚„ ìˆ˜ ìˆëŠ”ë° ì´ëŸ¬í•œ ì—­í• ì„ í•˜ëŠ” ê²ƒì„ ì¡°ê¸°ì¢…ë£Œ**ë¼ê³  ë¶€ë¥´ë©°, ë”¥ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì“°ì¸ë‹¤.

### ì¡°ê¸°ì¢…ë£Œ

* ê³¼ëŒ€ ì í•© ì‹œì‘ ì „ í›ˆë ¨ì„ ë¯¸ë¦¬ ì¤‘ì§€
* EarlyStopping ì½œë°± : ì¡°ê¸° ì¢…ë£Œ ì½œë°±
* patience ë§¤ê°œë³€ìˆ˜ : ê²€ì¦ ì ìˆ˜ ë¯¸ í–¥ìƒ ì‹œ ì¶”ê°€ ìˆ˜í–‰ ì—í¬í¬ ìˆ˜
* restore\_best\_weights=True : ê°€ì¥ ë‚®ì€ ê²€ì¦ ì†ì‹¤ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¡œ ë˜ëŒë¦¼

ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì‹¤ìŠµì„ ì§„í–‰ í•´ë³´ì

```python
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics='accuracy')

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5',
                                                save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True)

history = model.fit(train_scaled, train_target, epochs=20, verbose=0,
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])
print(early_stopping_cb.stopped_epoch)

# ê²°ê³¼
10
```

11ë²ˆì˜ ì—í¬í¬ ì‹¤í–‰ í›„ ì¢…ë£Œ ë˜ì—ˆë‹¤.
